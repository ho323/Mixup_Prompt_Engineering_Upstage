import os
import sys
import argparse
import json
import pandas as pd
from tqdm import tqdm
from dotenv import load_dotenv
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed
import torch

# Hugging Face Transformers
from transformers import logging as hf_logging
hf_logging.set_verbosity_error()  # Hugging Face warning ì œê±°
from datasets import Dataset
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq

# Load environment variables
load_dotenv()

# Add parent directory to path for evaluate import
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ------------------ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ------------------
def load_prompt_from_json(file_path):
    """JSON íŒŒì¼ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œë“œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        prompt_config = json.load(f)
    
    if "prompt" not in prompt_config:
        raise ValueError(f"File {file_path} does not contain 'prompt' key")
    
    prompt_data = prompt_config["prompt"]
    
    if "system_turns" not in prompt_data or "user_turns" not in prompt_data:
        raise ValueError(f"File {file_path} must contain 'system_turns' and 'user_turns' in 'prompt'")
    
    return prompt_data

# ------------------ Policy LM í•™ìŠµ ------------------
def train_policy_lm(train_csv, model_dir="policy_lm", epochs=3, batch_size=8, model_name="t5-small"):
    df = pd.read_csv(train_csv)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    df = df.fillna("")
    
    # í•™ìŠµìš© ë¬¸ì¥ ë³‘í•©: titleê³¼ sentence ëª¨ë‘ ì‚¬ìš©
    pairs = []
    for _, row in df.iterrows():
        if row.get("original_title") and row.get("answer_title"):
            pairs.append({"input": row["original_title"], "target": row["answer_title"]})
        if row.get("original_sentence") and row.get("answer_sentence"):
            pairs.append({"input": row["original_sentence"], "target": row["answer_sentence"]})
    
    if not pairs:
        raise ValueError("No valid training pairs found in CSV.")
    
    dataset = Dataset.from_list(pairs)
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    
    def preprocess(examples):
        inputs = ["Generate hint: " + text for text in examples["input"]]
        model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")
        labels = tokenizer(examples["target"], max_length=128, truncation=True, padding="max_length")
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs
    
    tokenized_dataset = dataset.map(preprocess, batched=True)
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    
    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)
    
    training_args = TrainingArguments(
        output_dir=model_dir,
        evaluation_strategy="no",
        per_device_train_batch_size=batch_size,
        num_train_epochs=epochs,
        save_strategy="epoch",
        logging_steps=50,
        save_total_limit=2,
        fp16=torch.cuda.is_available(),
        push_to_hub=False
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator
    )
    
    trainer.train()
    trainer.save_model(model_dir)
    print(f"âœ… Policy LM saved at {model_dir}")
    return tokenizer, model

# ------------------ íŒíŠ¸ ìƒì„± ------------------
def generate_hint(policy_tokenizer, policy_model, text, max_length=50, device="cuda" if torch.cuda.is_available() else "cpu"):
    policy_model.to(device)
    input_ids = policy_tokenizer("Generate hint: " + text, return_tensors="pt").input_ids.to(device)
    outputs = policy_model.generate(input_ids, max_length=max_length, num_beams=4)
    hint = policy_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return hint

# ------------------ ë©€í‹°í„´ ë³€í™˜ ------------------
def call_api_multi_turn(client, model, text, prompt_data, hint="", 
                        temp1=0.0, max_tokens1=None,
                        temp2=0.0, max_tokens2=None,
                        temp3=0.0, max_tokens3=None):
    """
    3í„´ ë³€í™˜ + DSP íŒíŠ¸ í¬í•¨
    1í„´ - ì˜ë¯¸ ë³´ì¡´, 
    2í„´ - ìì—°ìŠ¤ëŸ¬ì›€ + ì˜ë¯¸ ë³´ì¡´, 
    3í„´ - ì›ë¬¸ê³¼ ë¹„êµí•˜ì—¬ ë‚´ìš© ë³´ì¡´ í™•ì¸ ë° ë³´ê°•
    """
    try:
        system_turns = prompt_data["system_turns"]
        user_turns = prompt_data["user_turns"]
        
        # ì²« ë²ˆì§¸ í„´: íŒíŠ¸ í¬í•¨
        first_system_prompt = system_turns[0]
        first_user_template = user_turns[0]["template"]
        text_with_hint = f"{text}\nHint: {hint}" if hint else text
        first_user_prompt = first_user_template.format(text=text_with_hint)
        
        first_params = {"model": model, "temperature": temp1}
        if max_tokens1 is not None:
            first_params["max_tokens"] = max_tokens1
        
        resp_first = client.chat.completions.create(
            messages=[
                {"role": "system", "content": first_system_prompt},
                {"role": "user", "content": first_user_prompt}
            ],
            **first_params
        )
        first_result = resp_first.choices[0].message.content.strip()

        # ë‘ ë²ˆì§¸ í„´
        second_system_prompt = system_turns[1]
        second_user_template = user_turns[1]["template"]
        second_user_prompt = second_user_template.format(text=text, first_result=first_result)
        
        second_params = {"model": model, "temperature": temp2}
        if max_tokens2 is not None:
            second_params["max_tokens"] = max_tokens2
        
        resp_second = client.chat.completions.create(
            messages=[
                {"role": "system", "content": second_system_prompt},
                {"role": "user", "content": second_user_prompt}
            ],
            **second_params
        )
        second_result = resp_second.choices[0].message.content.strip()

        # ì„¸ ë²ˆì§¸ í„´
        third_system_prompt = system_turns[2]
        third_user_template = user_turns[2]["template"]
        third_user_prompt = third_user_template.format(text=text, second_result=second_result)
        
        third_params = {"model": model, "temperature": temp3}
        if max_tokens3 is not None:
            third_params["max_tokens"] = max_tokens3
        
        resp_third = client.chat.completions.create(
            messages=[
                {"role": "system", "content": third_system_prompt},
                {"role": "user", "content": third_user_prompt}
            ],
            **third_params
        )
        final_result = resp_third.choices[0].message.content.strip()
        return final_result

    except Exception as e:
        print(f"[ERROR] {text[:40]}... - {e}")
        return text  # fallback

# ------------------ í‰ê°€ ------------------
def run_evaluate(true_df_path, pred_df_path):
    from evaluate import evaluate
    true_df = pd.read_csv(true_df_path)
    pred_df = pd.read_csv(pred_df_path)
    result_df, summary_text, average_scores = evaluate(true_df, pred_df)
    return average_scores, summary_text

# ------------------ main ------------------
def main():
    parser = argparse.ArgumentParser(description="Multi-turn conversion + DSP")
    parser.add_argument("--train_csv", default="data/train_dataset.csv", help="Training CSV path")
    parser.add_argument("--input", default="data/test_dataset.csv", help="Input CSV path")
    parser.add_argument("--output", default="submission_DSP.csv", help="Output CSV path")
    parser.add_argument("--prompt", default="prompt_fit.json", help="Path to prompt JSON file (default: prompt_fit.json)")
    parser.add_argument("--model", default="solar-pro2", help="Model name (default: solar-pro2)")
    parser.add_argument("--max_workers", type=int, default=8, help="Number of parallel workers (default: 8)")
    parser.add_argument("--policy_model_dir", default="policy_lm", help="Policy LM model directory")
    parser.add_argument("--train_policy_lm", action="store_true", help="Train Policy LM")
    
    # 1í„´ íŒŒë¼ë¯¸í„°
    parser.add_argument("--temp1", type=float, default=0.0, help="Temperature for 1st turn (default: 0.0)")
    parser.add_argument("--max_tokens1", type=int, default=None, help="Max tokens for 1st turn (default: None)")
    
    # 2í„´ íŒŒë¼ë¯¸í„°
    parser.add_argument("--temp2", type=float, default=0.0, help="Temperature for 2nd turn (default: 0.0)")
    parser.add_argument("--max_tokens2", type=int, default=None, help="Max tokens for 2nd turn (default: None)")
    
    # 3í„´ íŒŒë¼ë¯¸í„°
    parser.add_argument("--temp3", type=float, default=0.0, help="Temperature for 3rd turn (default: 0.0)")
    parser.add_argument("--max_tokens3", type=int, default=None, help="Max tokens for 3rd turn (default: None)")
    
    parser.add_argument("--evaluate", action="store_true", help="Run evaluation after generation")
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1. Policy LM í•™ìŠµ
    if args.train_policy_lm or not os.path.exists(args.policy_model_dir):
        print("ğŸ”„ Training Policy LM...")
        tokenizer, policy_model = train_policy_lm(args.train_csv, model_dir=args.policy_model_dir)
    else:
        print(f"ğŸ“‚ Loading Policy LM from {args.policy_model_dir}...")
        tokenizer = T5Tokenizer.from_pretrained(args.policy_model_dir)
        policy_model = T5ForConditionalGeneration.from_pretrained(args.policy_model_dir)

    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
    df_test = pd.read_csv(args.input)
    if "original_sentence" not in df_test.columns or "id" not in df_test.columns:
        raise ValueError("Input CSV must contain 'original_sentence' and 'id' columns")

    # 3. Upstage API client
    api_key = os.getenv("UPSTAGE_API_KEY")
    if not api_key:
        raise ValueError("UPSTAGE_API_KEY missing.")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1")

    # 4. í”„ë¡¬í”„íŠ¸ ë¡œë“œ
    prompt_data = load_prompt_from_json(args.prompt)
    print(f"ğŸ“ Loaded prompt from {args.prompt}")

    # 5. ë³€í™˜
    results = {}
    with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
        future_map = {}
        for idx, text in enumerate(df_test["original_sentence"].astype(str)):
            hint = generate_hint(tokenizer, policy_model, text, device=device)
            future = executor.submit(call_api_multi_turn, client, args.model, text, prompt_data, hint,
                                   args.temp1, args.max_tokens1,
                                   args.temp2, args.max_tokens2,
                                   args.temp3, args.max_tokens3)
            future_map[future] = idx

        for future in tqdm(as_completed(future_map), total=len(future_map), desc="Generating"):
            idx = future_map[future]
            results[idx] = future.result()

    # 6. ì €ì¥
    final_df = pd.DataFrame([
        {"id": df_test.iloc[i]["id"], "original_sentence": df_test.iloc[i]["original_sentence"],
         "answer_sentence": results[i] if results[i] else df_test.iloc[i]["original_sentence"]}
        for i in range(len(df_test))
    ])
    final_df.to_csv(args.output, index=False)
    print(f"ğŸ’¾ Output saved: {args.output}")

    # 7. í‰ê°€
    if args.evaluate:
        print("\nğŸ“Š Running evaluation...")
        scores, summary = run_evaluate(args.input, args.output)
        eval_result_path = args.output.replace(".csv", "_eval.json")
        json.dump({"scores": scores, "summary": summary}, open(eval_result_path, "w", encoding="utf-8"), ensure_ascii=False, indent=2)
        print("\nğŸ Evaluation Completed!")
        print(f"ğŸ“Œ Saved: {eval_result_path}")
        print("\n===== ê²°ê³¼ ìš”ì•½ =====")
        print(summary)
        print("\n===================")

if __name__ == "__main__":
    main()

